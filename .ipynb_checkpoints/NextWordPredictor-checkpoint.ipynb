{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open('corpus.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Next Word Prediction is also called Language Modeling. It is the task of predicting what word comes next. It is one of the fundamental tasks of NLP and has many applications. You might be using it daily when you write texts or emails without realizing it.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = corpus.replace(\"\\n\",\"\")\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'next word prediction is also called language modeling it is the task of predicting what word comes next it is one of the fundamental tasks of nlp and has many applications you might be using it daily when you write texts or emails without realizing it '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "corpus = corpus.lower()\n",
    "clean_corpus = re.sub('[^a-z0-9]+',' ', corpus)\n",
    "clean_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/aditya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['next',\n",
       " 'word',\n",
       " 'prediction',\n",
       " 'is',\n",
       " 'also',\n",
       " 'called',\n",
       " 'language',\n",
       " 'modeling',\n",
       " 'it',\n",
       " 'is',\n",
       " 'the',\n",
       " 'task',\n",
       " 'of',\n",
       " 'predicting',\n",
       " 'what',\n",
       " 'word',\n",
       " 'comes',\n",
       " 'next',\n",
       " 'it',\n",
       " 'is',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'fundamental',\n",
       " 'tasks',\n",
       " 'of',\n",
       " 'nlp',\n",
       " 'and',\n",
       " 'has',\n",
       " 'many',\n",
       " 'applications',\n",
       " 'you',\n",
       " 'might',\n",
       " 'be',\n",
       " 'using',\n",
       " 'it',\n",
       " 'daily',\n",
       " 'when',\n",
       " 'you',\n",
       " 'write',\n",
       " 'texts',\n",
       " 'or',\n",
       " 'emails',\n",
       " 'without',\n",
       " 'realizing',\n",
       " 'it']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(clean_corpus)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = 3\n",
    "text_sequences = []\n",
    "for i in range(train_len, len(tokens)+1):\n",
    "    seq = tokens[i-train_len:i]\n",
    "    text_sequences.append(seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['next', 'word', 'prediction'],\n",
       " ['word', 'prediction', 'is'],\n",
       " ['prediction', 'is', 'also'],\n",
       " ['is', 'also', 'called'],\n",
       " ['also', 'called', 'language'],\n",
       " ['called', 'language', 'modeling'],\n",
       " ['language', 'modeling', 'it'],\n",
       " ['modeling', 'it', 'is'],\n",
       " ['it', 'is', 'the'],\n",
       " ['is', 'the', 'task'],\n",
       " ['the', 'task', 'of'],\n",
       " ['task', 'of', 'predicting'],\n",
       " ['of', 'predicting', 'what'],\n",
       " ['predicting', 'what', 'word'],\n",
       " ['what', 'word', 'comes'],\n",
       " ['word', 'comes', 'next'],\n",
       " ['comes', 'next', 'it'],\n",
       " ['next', 'it', 'is'],\n",
       " ['it', 'is', 'one'],\n",
       " ['is', 'one', 'of'],\n",
       " ['one', 'of', 'the'],\n",
       " ['of', 'the', 'fundamental'],\n",
       " ['the', 'fundamental', 'tasks'],\n",
       " ['fundamental', 'tasks', 'of'],\n",
       " ['tasks', 'of', 'nlp'],\n",
       " ['of', 'nlp', 'and'],\n",
       " ['nlp', 'and', 'has'],\n",
       " ['and', 'has', 'many'],\n",
       " ['has', 'many', 'applications'],\n",
       " ['many', 'applications', 'you'],\n",
       " ['applications', 'you', 'might'],\n",
       " ['you', 'might', 'be'],\n",
       " ['might', 'be', 'using'],\n",
       " ['be', 'using', 'it'],\n",
       " ['using', 'it', 'daily'],\n",
       " ['it', 'daily', 'when'],\n",
       " ['daily', 'when', 'you'],\n",
       " ['when', 'you', 'write'],\n",
       " ['you', 'write', 'texts'],\n",
       " ['write', 'texts', 'or'],\n",
       " ['texts', 'or', 'emails'],\n",
       " ['or', 'emails', 'without'],\n",
       " ['emails', 'without', 'realizing'],\n",
       " ['without', 'realizing', 'it']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7, 6, 8],\n",
       " [6, 8, 2],\n",
       " [8, 2, 9],\n",
       " [2, 9, 10],\n",
       " [9, 10, 11],\n",
       " [10, 11, 12],\n",
       " [11, 12, 1],\n",
       " [12, 1, 2],\n",
       " [1, 2, 4],\n",
       " [2, 4, 13],\n",
       " [4, 13, 3],\n",
       " [13, 3, 14],\n",
       " [3, 14, 15],\n",
       " [14, 15, 6],\n",
       " [15, 6, 16],\n",
       " [6, 16, 7],\n",
       " [16, 7, 1],\n",
       " [7, 1, 2],\n",
       " [1, 2, 17],\n",
       " [2, 17, 3],\n",
       " [17, 3, 4],\n",
       " [3, 4, 18],\n",
       " [4, 18, 19],\n",
       " [18, 19, 3],\n",
       " [19, 3, 20],\n",
       " [3, 20, 21],\n",
       " [20, 21, 22],\n",
       " [21, 22, 23],\n",
       " [22, 23, 24],\n",
       " [23, 24, 5],\n",
       " [24, 5, 25],\n",
       " [5, 25, 26],\n",
       " [25, 26, 27],\n",
       " [26, 27, 1],\n",
       " [27, 1, 28],\n",
       " [1, 28, 29],\n",
       " [28, 29, 5],\n",
       " [29, 5, 30],\n",
       " [5, 30, 31],\n",
       " [30, 31, 32],\n",
       " [31, 32, 33],\n",
       " [32, 33, 34],\n",
       " [33, 34, 35],\n",
       " [34, 35, 1]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_sequences)\n",
    "sequences = tokenizer.texts_to_sequences(text_sequences)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences=np.asarray(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_counts) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7,  6],\n",
       "       [ 6,  8],\n",
       "       [ 8,  2],\n",
       "       [ 2,  9],\n",
       "       [ 9, 10],\n",
       "       [10, 11],\n",
       "       [11, 12],\n",
       "       [12,  1],\n",
       "       [ 1,  2],\n",
       "       [ 2,  4],\n",
       "       [ 4, 13],\n",
       "       [13,  3],\n",
       "       [ 3, 14],\n",
       "       [14, 15],\n",
       "       [15,  6],\n",
       "       [ 6, 16],\n",
       "       [16,  7],\n",
       "       [ 7,  1],\n",
       "       [ 1,  2],\n",
       "       [ 2, 17],\n",
       "       [17,  3],\n",
       "       [ 3,  4],\n",
       "       [ 4, 18],\n",
       "       [18, 19],\n",
       "       [19,  3],\n",
       "       [ 3, 20],\n",
       "       [20, 21],\n",
       "       [21, 22],\n",
       "       [22, 23],\n",
       "       [23, 24],\n",
       "       [24,  5],\n",
       "       [ 5, 25],\n",
       "       [25, 26],\n",
       "       [26, 27],\n",
       "       [27,  1],\n",
       "       [ 1, 28],\n",
       "       [28, 29],\n",
       "       [29,  5],\n",
       "       [ 5, 30],\n",
       "       [30, 31],\n",
       "       [31, 32],\n",
       "       [32, 33],\n",
       "       [33, 34],\n",
       "       [34, 35]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs = sequences[:,:-1]\n",
    "train_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length = train_inputs.shape[1]\n",
    "seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8,  2,  9, 10, 11, 12,  1,  2,  4, 13,  3, 14, 15,  6, 16,  7,  1,\n",
       "        2, 17,  3,  4, 18, 19,  3, 20, 21, 22, 23, 24,  5, 25, 26, 27,  1,\n",
       "       28, 29,  5, 30, 31, 32, 33, 34, 35,  1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets = sequences[:,-1]\n",
    "train_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets = to_categorical(train_targets,num_classes=vocab_size)\n",
    "train_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as Adam\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cla"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
